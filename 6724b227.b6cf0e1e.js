(window.webpackJsonp=window.webpackJsonp||[]).push([[21],{127:function(e,t,n){"use strict";n.d(t,"a",(function(){return u})),n.d(t,"b",(function(){return d}));var r=n(0),a=n.n(r);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function c(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function p(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?c(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):c(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=a.a.createContext({}),s=function(e){var t=a.a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):p(p({},t),e)),n},u=function(e){var t=s(e.components);return a.a.createElement(l.Provider,{value:t},e.children)},b={inlineCode:"code",wrapper:function(e){var t=e.children;return a.a.createElement(a.a.Fragment,{},t)}},m=a.a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,c=e.parentName,l=i(e,["components","mdxType","originalType","parentName"]),u=s(n),m=r,d=u["".concat(c,".").concat(m)]||u[m]||b[m]||o;return n?a.a.createElement(d,p(p({ref:t},l),{},{components:n})):a.a.createElement(d,p({ref:t},l))}));function d(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,c=new Array(o);c[0]=m;var p={};for(var i in t)hasOwnProperty.call(t,i)&&(p[i]=t[i]);p.originalType=e,p.mdxType="string"==typeof e?e:r,c[1]=p;for(var l=2;l<o;l++)c[l]=n[l];return a.a.createElement.apply(null,c)}return a.a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},214:function(e,t,n){"use strict";n.r(t),t.default=n.p+"assets/images/306_optimizer_5_0-11a4d14857ce695e9cb58b699780c122.png"},215:function(e,t,n){"use strict";n.r(t),t.default=n.p+"assets/images/306_optimizer_15_1-39e6d171a71b56d29c3900c0cce190ac.png"},91:function(e,t,n){"use strict";n.r(t),n.d(t,"frontMatter",(function(){return c})),n.d(t,"metadata",(function(){return p})),n.d(t,"toc",(function(){return i})),n.d(t,"default",(function(){return s}));var r=n(3),a=n(7),o=(n(0),n(127)),c={title:"206 Optimizers"},p={unversionedId:"ai/Docker-PyTorch-Tutorial/306_optimizer",id:"ai/Docker-PyTorch-Tutorial/306_optimizer",isDocsHomePage:!1,title:"206 Optimizers",description:"View more, visit my tutorial page//mofanpy.com/tutorials/",source:"@site/docs/ai/Docker-PyTorch-Tutorial/306_optimizer.md",slug:"/ai/Docker-PyTorch-Tutorial/306_optimizer",permalink:"/docs/ai/Docker-PyTorch-Tutorial/306_optimizer",version:"current",sidebar:"AI",previous:{title:"305 Batch Train",permalink:"/docs/ai/Docker-PyTorch-Tutorial/305_batch_train"},next:{title:"401 CNN",permalink:"/docs/ai/Docker-PyTorch-Tutorial/401_CNN"}},i=[{value:"Generate some fake data",id:"generate-some-fake-data",children:[]},{value:"Put dataset into torch dataset",id:"put-dataset-into-torch-dataset",children:[]},{value:"Default network",id:"default-network",children:[]},{value:"Different nets",id:"different-nets",children:[]},{value:"Different optimizers",id:"different-optimizers",children:[]}],l={toc:i};function s(e){var t=e.components,c=Object(a.a)(e,["components"]);return Object(o.b)("wrapper",Object(r.a)({},l,c,{components:t,mdxType:"MDXLayout"}),Object(o.b)("p",null,"View more, visit my tutorial page: ",Object(o.b)("a",Object(r.a)({parentName:"p"},{href:"https://mofanpy.com/tutorials/"}),"https://mofanpy.com/tutorials/"),"\nMy Youtube Channel: ",Object(o.b)("a",Object(r.a)({parentName:"p"},{href:"https://www.youtube.com/user/MorvanZhou"}),"https://www.youtube.com/user/MorvanZhou")),Object(o.b)("p",null,"Dependencies:"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"torch: 0.1.11"),Object(o.b)("li",{parentName:"ul"},"matplotlib")),Object(o.b)("pre",null,Object(o.b)("code",Object(r.a)({parentName:"pre"},{className:"language-python"}),"import torch\nimport torch.utils.data as Data\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport matplotlib.pyplot as plt\n%matplotlib inline\n")),Object(o.b)("pre",null,Object(o.b)("code",Object(r.a)({parentName:"pre"},{className:"language-python"}),"torch.manual_seed(1)    # reproducible\n")),Object(o.b)("pre",null,Object(o.b)("code",Object(r.a)({parentName:"pre"},{}),"<torch._C.Generator at 0x7f502d43b930>\n")),Object(o.b)("pre",null,Object(o.b)("code",Object(r.a)({parentName:"pre"},{className:"language-python"}),"LR = 0.01\nBATCH_SIZE = 32\nEPOCH = 12\n")),Object(o.b)("h3",{id:"generate-some-fake-data"},"Generate some fake data"),Object(o.b)("pre",null,Object(o.b)("code",Object(r.a)({parentName:"pre"},{className:"language-python"}),"# fake dataset\nx = torch.unsqueeze(torch.linspace(-1, 1, 1000), dim=1)\ny = x.pow(2) + 0.1*torch.normal(torch.zeros(*x.size()))\n\n# plot dataset\nplt.scatter(x.numpy(), y.numpy())\nplt.show()\n")),Object(o.b)("p",null,Object(o.b)("img",{alt:"png",src:n(214).default})),Object(o.b)("h3",{id:"put-dataset-into-torch-dataset"},"Put dataset into torch dataset"),Object(o.b)("pre",null,Object(o.b)("code",Object(r.a)({parentName:"pre"},{className:"language-python"}),"torch_dataset = Data.TensorDataset(data_tensor=x, target_tensor=y)\nloader = Data.DataLoader(\n    dataset=torch_dataset, \n    batch_size=BATCH_SIZE, \n    shuffle=True, num_workers=2,)\n")),Object(o.b)("h3",{id:"default-network"},"Default network"),Object(o.b)("pre",null,Object(o.b)("code",Object(r.a)({parentName:"pre"},{className:"language-python"}),"class Net(torch.nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.hidden = torch.nn.Linear(1, 20)   # hidden layer\n        self.predict = torch.nn.Linear(20, 1)   # output layer\n\n    def forward(self, x):\n        x = F.relu(self.hidden(x))      # activation function for hidden layer\n        x = self.predict(x)             # linear output\n        return x\n")),Object(o.b)("h3",{id:"different-nets"},"Different nets"),Object(o.b)("pre",null,Object(o.b)("code",Object(r.a)({parentName:"pre"},{className:"language-python"}),"net_SGD         = Net()\nnet_Momentum    = Net()\nnet_RMSprop     = Net()\nnet_Adam        = Net()\nnets = [net_SGD, net_Momentum, net_RMSprop, net_Adam]\n")),Object(o.b)("h3",{id:"different-optimizers"},"Different optimizers"),Object(o.b)("pre",null,Object(o.b)("code",Object(r.a)({parentName:"pre"},{className:"language-python"}),"opt_SGD         = torch.optim.SGD(net_SGD.parameters(), lr=LR)\nopt_Momentum    = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=0.8)\nopt_RMSprop     = torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=0.9)\nopt_Adam        = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(0.9, 0.99))\noptimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]\n")),Object(o.b)("pre",null,Object(o.b)("code",Object(r.a)({parentName:"pre"},{className:"language-python"}),"loss_func = torch.nn.MSELoss()\nlosses_his = [[], [], [], []]   # record loss\n")),Object(o.b)("pre",null,Object(o.b)("code",Object(r.a)({parentName:"pre"},{className:"language-python"}),"# training\nfor epoch in range(EPOCH):\n    print('Epoch: ', epoch)\n    for step, (batch_x, batch_y) in enumerate(loader):          # for each training step\n        b_x = Variable(batch_x)\n        b_y = Variable(batch_y)\n\n        for net, opt, l_his in zip(nets, optimizers, losses_his):\n            output = net(b_x)              # get output for every net\n            loss = loss_func(output, b_y)  # compute loss for every net\n            opt.zero_grad()                # clear gradients for next train\n            loss.backward()                # backpropagation, compute gradients\n            opt.step()                     # apply gradients\n            l_his.append(loss.item())     # loss recoder\n\nlabels = ['SGD', 'Momentum', 'RMSprop', 'Adam']\nfor i, l_his in enumerate(losses_his):\n    plt.plot(l_his, label=labels[i])\nplt.legend(loc='best')\nplt.xlabel('Steps')\nplt.ylabel('Loss')\nplt.ylim((0, 0.2))\nplt.show()\n")),Object(o.b)("pre",null,Object(o.b)("code",Object(r.a)({parentName:"pre"},{}),"Epoch:  0\nEpoch:  1\nEpoch:  2\nEpoch:  3\nEpoch:  4\nEpoch:  5\nEpoch:  6\nEpoch:  7\nEpoch:  8\nEpoch:  9\nEpoch:  10\nEpoch:  11\n")),Object(o.b)("p",null,Object(o.b)("img",{alt:"png",src:n(215).default})),Object(o.b)("pre",null,Object(o.b)("code",Object(r.a)({parentName:"pre"},{className:"language-python"}),"")))}s.isMDXComponent=!0}}]);