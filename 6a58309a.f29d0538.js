(window.webpackJsonp=window.webpackJsonp||[]).push([[24],{130:function(n,e,r){"use strict";r.d(e,"a",(function(){return l})),r.d(e,"b",(function(){return u}));var p=r(0),t=r.n(p);function E(n,e,r){return e in n?Object.defineProperty(n,e,{value:r,enumerable:!0,configurable:!0,writable:!0}):n[e]=r,n}function a(n,e){var r=Object.keys(n);if(Object.getOwnPropertySymbols){var p=Object.getOwnPropertySymbols(n);e&&(p=p.filter((function(e){return Object.getOwnPropertyDescriptor(n,e).enumerable}))),r.push.apply(r,p)}return r}function _(n){for(var e=1;e<arguments.length;e++){var r=null!=arguments[e]?arguments[e]:{};e%2?a(Object(r),!0).forEach((function(e){E(n,e,r[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(n,Object.getOwnPropertyDescriptors(r)):a(Object(r)).forEach((function(e){Object.defineProperty(n,e,Object.getOwnPropertyDescriptor(r,e))}))}return n}function o(n,e){if(null==n)return{};var r,p,t=function(n,e){if(null==n)return{};var r,p,t={},E=Object.keys(n);for(p=0;p<E.length;p++)r=E[p],e.indexOf(r)>=0||(t[r]=n[r]);return t}(n,e);if(Object.getOwnPropertySymbols){var E=Object.getOwnPropertySymbols(n);for(p=0;p<E.length;p++)r=E[p],e.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(n,r)&&(t[r]=n[r])}return t}var c=t.a.createContext({}),i=function(n){var e=t.a.useContext(c),r=e;return n&&(r="function"==typeof n?n(e):_(_({},e),n)),r},l=function(n){var e=i(n.components);return t.a.createElement(c.Provider,{value:e},n.children)},s={inlineCode:"code",wrapper:function(n){var e=n.children;return t.a.createElement(t.a.Fragment,{},e)}},m=t.a.forwardRef((function(n,e){var r=n.components,p=n.mdxType,E=n.originalType,a=n.parentName,c=o(n,["components","mdxType","originalType","parentName"]),l=i(r),m=p,u=l["".concat(a,".").concat(m)]||l[m]||s[m]||E;return r?t.a.createElement(u,_(_({ref:e},c),{},{components:r})):t.a.createElement(u,_({ref:e},c))}));function u(n,e){var r=arguments,p=e&&e.mdxType;if("string"==typeof n||p){var E=r.length,a=new Array(E);a[0]=m;var _={};for(var o in e)hasOwnProperty.call(e,o)&&(_[o]=e[o]);_.originalType=n,_.mdxType="string"==typeof n?n:p,a[1]=_;for(var c=2;c<E;c++)a[c]=r[c];return t.a.createElement.apply(null,a)}return t.a.createElement.apply(null,r)}m.displayName="MDXCreateElement"},95:function(n,e,r){"use strict";r.r(e),r.d(e,"frontMatter",(function(){return a})),r.d(e,"metadata",(function(){return _})),r.d(e,"toc",(function(){return o})),r.d(e,"default",(function(){return i}));var p=r(3),t=r(7),E=(r(0),r(130)),a={title:"405 DQN Reinforcement Learning"},_={unversionedId:"ai/Docker-PyTorch-Tutorial/405_DQN_Reinforcement_learning",id:"ai/Docker-PyTorch-Tutorial/405_DQN_Reinforcement_learning",isDocsHomePage:!1,title:"405 DQN Reinforcement Learning",description:"View more, visit my tutorial page//mofanpy.com/tutorials/",source:"@site/docs/ai/Docker-PyTorch-Tutorial/405_DQN_Reinforcement_learning.md",slug:"/ai/Docker-PyTorch-Tutorial/405_DQN_Reinforcement_learning",permalink:"/docs/ai/Docker-PyTorch-Tutorial/405_DQN_Reinforcement_learning",version:"current",sidebar:"AI",previous:{title:"404 Autoencoder",permalink:"/docs/ai/Docker-PyTorch-Tutorial/404_autoencoder"},next:{title:"406 GAN",permalink:"/docs/ai/Docker-PyTorch-Tutorial/406_GAN"}},o=[],c={toc:o};function i(n){var e=n.components,r=Object(t.a)(n,["components"]);return Object(E.b)("wrapper",Object(p.a)({},c,r,{components:e,mdxType:"MDXLayout"}),Object(E.b)("p",null,"View more, visit my tutorial page: ",Object(E.b)("a",Object(p.a)({parentName:"p"},{href:"https://mofanpy.com/tutorials/"}),"https://mofanpy.com/tutorials/"),"\nMy Youtube Channel: ",Object(E.b)("a",Object(p.a)({parentName:"p"},{href:"https://www.youtube.com/user/MorvanZhou"}),"https://www.youtube.com/user/MorvanZhou"),"\nMore about Reinforcement learning: ",Object(E.b)("a",Object(p.a)({parentName:"p"},{href:"https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/"}),"https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/")),Object(E.b)("p",null,"Dependencies:"),Object(E.b)("ul",null,Object(E.b)("li",{parentName:"ul"},"torch: 0.1.11"),Object(E.b)("li",{parentName:"ul"},"gym: 0.8.1"),Object(E.b)("li",{parentName:"ul"},"numpy")),Object(E.b)("pre",null,Object(E.b)("code",Object(p.a)({parentName:"pre"},{className:"language-python"}),"import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport numpy as np\nimport gym\n")),Object(E.b)("pre",null,Object(E.b)("code",Object(p.a)({parentName:"pre"},{className:"language-python"}),"# Hyper Parameters\nBATCH_SIZE = 32\nLR = 0.01                   # learning rate\nEPSILON = 0.9               # greedy policy\nGAMMA = 0.9                 # reward discount\nTARGET_REPLACE_ITER = 100   # target update frequency\nMEMORY_CAPACITY = 2000\nenv = gym.make('CartPole-v0')\nenv = env.unwrapped\nN_ACTIONS = env.action_space.n\nN_STATES = env.observation_space.shape[0]\n")),Object(E.b)("pre",null,Object(E.b)("code",Object(p.a)({parentName:"pre"},{}),"[2017-06-20 22:23:40,418] Making new env: CartPole-v0\n")),Object(E.b)("pre",null,Object(E.b)("code",Object(p.a)({parentName:"pre"},{className:"language-python"}),"class Net(nn.Module):\n    def __init__(self, ):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(N_STATES, 10)\n        self.fc1.weight.data.normal_(0, 0.1)   # initialization\n        self.out = nn.Linear(10, N_ACTIONS)\n        self.out.weight.data.normal_(0, 0.1)   # initialization\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = F.relu(x)\n        actions_value = self.out(x)\n        return actions_value\n")),Object(E.b)("pre",null,Object(E.b)("code",Object(p.a)({parentName:"pre"},{className:"language-python"}),"class DQN(object):\n    def __init__(self):\n        self.eval_net, self.target_net = Net(), Net()\n\n        self.learn_step_counter = 0                                     # for target updating\n        self.memory_counter = 0                                         # for storing memory\n        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))     # initialize memory\n        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)\n        self.loss_func = nn.MSELoss()\n\n    def choose_action(self, x):\n        x = Variable(torch.unsqueeze(torch.FloatTensor(x), 0))\n        # input only one sample\n        if np.random.uniform() < EPSILON:   # greedy\n            actions_value = self.eval_net.forward(x)\n            action = torch.max(actions_value, 1)[1].data.numpy()[0, 0]     # return the argmax\n        else:   # random\n            action = np.random.randint(0, N_ACTIONS)\n        return action\n\n    def store_transition(self, s, a, r, s_):\n        transition = np.hstack((s, [a, r], s_))\n        # replace the old memory with new memory\n        index = self.memory_counter % MEMORY_CAPACITY\n        self.memory[index, :] = transition\n        self.memory_counter += 1\n\n    def learn(self):\n        # target parameter update\n        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n            self.target_net.load_state_dict(self.eval_net.state_dict())\n        self.learn_step_counter += 1\n\n        # sample batch transitions\n        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n        b_memory = self.memory[sample_index, :]\n        b_s = Variable(torch.FloatTensor(b_memory[:, :N_STATES]))\n        b_a = Variable(torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int)))\n        b_r = Variable(torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2]))\n        b_s_ = Variable(torch.FloatTensor(b_memory[:, -N_STATES:]))\n\n        # q_eval w.r.t the action in experience\n        q_eval = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n        q_next = self.target_net(b_s_).detach()     # detach from graph, don't backpropagate\n        q_target = b_r + GAMMA * q_next.max(1)[0]   # shape (batch, 1)\n        loss = self.loss_func(q_eval, q_target)\n\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n")),Object(E.b)("pre",null,Object(E.b)("code",Object(p.a)({parentName:"pre"},{className:"language-python"}),"dqn = DQN()\n")),Object(E.b)("pre",null,Object(E.b)("code",Object(p.a)({parentName:"pre"},{className:"language-python"}),"\nprint('\\nCollecting experience...')\nfor i_episode in range(400):\n    s = env.reset()\n    ep_r = 0\n    while True:\n        env.render()\n        a = dqn.choose_action(s)\n\n        # take action\n        s_, r, done, info = env.step(a)\n\n        # modify the reward\n        x, x_dot, theta, theta_dot = s_\n        r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.8\n        r2 = (env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5\n        r = r1 + r2\n\n        dqn.store_transition(s, a, r, s_)\n\n        ep_r += r\n        if dqn.memory_counter > MEMORY_CAPACITY:\n            dqn.learn()\n            if done:\n                print('Ep: ', i_episode,\n                      '| Ep_r: ', round(ep_r, 2))\n\n        if done:\n            break\n        s = s_\n")),Object(E.b)("pre",null,Object(E.b)("code",Object(p.a)({parentName:"pre"},{}),"Collecting experience...\nEp:  201 | Ep_r:  1.59\nEp:  202 | Ep_r:  4.18\nEp:  203 | Ep_r:  2.73\nEp:  204 | Ep_r:  1.97\nEp:  205 | Ep_r:  1.18\nEp:  206 | Ep_r:  0.86\nEp:  207 | Ep_r:  2.88\nEp:  208 | Ep_r:  1.63\nEp:  209 | Ep_r:  3.91\nEp:  210 | Ep_r:  3.6\nEp:  211 | Ep_r:  0.98\nEp:  212 | Ep_r:  3.85\nEp:  213 | Ep_r:  1.81\nEp:  214 | Ep_r:  2.32\nEp:  215 | Ep_r:  3.75\nEp:  216 | Ep_r:  3.53\nEp:  217 | Ep_r:  4.75\nEp:  218 | Ep_r:  2.4\nEp:  219 | Ep_r:  0.64\nEp:  220 | Ep_r:  1.15\nEp:  221 | Ep_r:  2.3\nEp:  222 | Ep_r:  7.37\nEp:  223 | Ep_r:  1.25\nEp:  224 | Ep_r:  5.02\nEp:  225 | Ep_r:  10.29\nEp:  226 | Ep_r:  17.54\nEp:  227 | Ep_r:  36.2\nEp:  228 | Ep_r:  6.61\nEp:  229 | Ep_r:  10.04\nEp:  230 | Ep_r:  55.19\nEp:  231 | Ep_r:  10.03\nEp:  232 | Ep_r:  13.25\nEp:  233 | Ep_r:  8.75\nEp:  234 | Ep_r:  3.83\nEp:  235 | Ep_r:  -0.92\nEp:  236 | Ep_r:  5.12\nEp:  237 | Ep_r:  3.56\nEp:  238 | Ep_r:  5.69\nEp:  239 | Ep_r:  8.43\nEp:  240 | Ep_r:  29.27\nEp:  241 | Ep_r:  17.95\nEp:  242 | Ep_r:  44.77\nEp:  243 | Ep_r:  98.0\nEp:  244 | Ep_r:  38.78\nEp:  245 | Ep_r:  45.02\nEp:  246 | Ep_r:  27.73\nEp:  247 | Ep_r:  36.96\nEp:  248 | Ep_r:  48.98\nEp:  249 | Ep_r:  111.36\nEp:  250 | Ep_r:  95.61\nEp:  251 | Ep_r:  149.77\nEp:  252 | Ep_r:  29.96\nEp:  253 | Ep_r:  2.79\nEp:  254 | Ep_r:  20.1\nEp:  255 | Ep_r:  24.25\nEp:  256 | Ep_r:  3074.75\nEp:  257 | Ep_r:  1258.49\nEp:  258 | Ep_r:  127.39\nEp:  259 | Ep_r:  283.46\nEp:  260 | Ep_r:  166.96\nEp:  261 | Ep_r:  101.71\nEp:  262 | Ep_r:  63.45\nEp:  263 | Ep_r:  288.94\nEp:  264 | Ep_r:  130.49\nEp:  265 | Ep_r:  207.05\nEp:  266 | Ep_r:  183.71\nEp:  267 | Ep_r:  142.75\nEp:  268 | Ep_r:  126.53\nEp:  269 | Ep_r:  310.79\nEp:  270 | Ep_r:  863.2\nEp:  271 | Ep_r:  365.12\nEp:  272 | Ep_r:  659.52\nEp:  273 | Ep_r:  103.98\nEp:  274 | Ep_r:  554.83\nEp:  275 | Ep_r:  246.01\nEp:  276 | Ep_r:  332.23\nEp:  277 | Ep_r:  323.35\nEp:  278 | Ep_r:  278.71\nEp:  279 | Ep_r:  613.6\nEp:  280 | Ep_r:  152.21\nEp:  281 | Ep_r:  402.02\nEp:  282 | Ep_r:  351.4\nEp:  283 | Ep_r:  115.87\nEp:  284 | Ep_r:  163.26\nEp:  285 | Ep_r:  631.0\nEp:  286 | Ep_r:  263.47\nEp:  287 | Ep_r:  511.21\nEp:  288 | Ep_r:  337.18\nEp:  289 | Ep_r:  819.76\nEp:  290 | Ep_r:  190.83\nEp:  291 | Ep_r:  442.98\nEp:  292 | Ep_r:  537.24\nEp:  293 | Ep_r:  1101.12\nEp:  294 | Ep_r:  178.42\nEp:  295 | Ep_r:  225.61\nEp:  296 | Ep_r:  252.62\nEp:  297 | Ep_r:  617.5\nEp:  298 | Ep_r:  617.8\nEp:  299 | Ep_r:  244.01\nEp:  300 | Ep_r:  687.91\nEp:  301 | Ep_r:  618.51\nEp:  302 | Ep_r:  1405.07\nEp:  303 | Ep_r:  456.95\nEp:  304 | Ep_r:  340.33\nEp:  305 | Ep_r:  502.91\nEp:  306 | Ep_r:  441.21\nEp:  307 | Ep_r:  255.81\nEp:  308 | Ep_r:  403.03\nEp:  309 | Ep_r:  229.1\nEp:  310 | Ep_r:  308.49\nEp:  311 | Ep_r:  165.37\nEp:  312 | Ep_r:  153.76\nEp:  313 | Ep_r:  442.05\nEp:  314 | Ep_r:  229.23\nEp:  315 | Ep_r:  128.52\nEp:  316 | Ep_r:  358.18\nEp:  317 | Ep_r:  319.03\nEp:  318 | Ep_r:  381.76\nEp:  319 | Ep_r:  199.19\nEp:  320 | Ep_r:  418.63\nEp:  321 | Ep_r:  223.95\nEp:  322 | Ep_r:  222.37\nEp:  323 | Ep_r:  405.4\nEp:  324 | Ep_r:  311.32\nEp:  325 | Ep_r:  184.85\nEp:  326 | Ep_r:  1026.71\nEp:  327 | Ep_r:  252.41\nEp:  328 | Ep_r:  224.93\nEp:  329 | Ep_r:  620.02\nEp:  330 | Ep_r:  174.54\nEp:  331 | Ep_r:  782.45\nEp:  332 | Ep_r:  263.79\nEp:  333 | Ep_r:  178.63\nEp:  334 | Ep_r:  242.84\nEp:  335 | Ep_r:  635.43\nEp:  336 | Ep_r:  668.89\nEp:  337 | Ep_r:  265.42\nEp:  338 | Ep_r:  207.81\nEp:  339 | Ep_r:  293.09\nEp:  340 | Ep_r:  530.23\nEp:  341 | Ep_r:  479.26\nEp:  342 | Ep_r:  559.77\nEp:  343 | Ep_r:  241.39\nEp:  344 | Ep_r:  158.83\nEp:  345 | Ep_r:  1510.69\nEp:  346 | Ep_r:  425.17\nEp:  347 | Ep_r:  266.94\nEp:  348 | Ep_r:  166.08\nEp:  349 | Ep_r:  630.52\nEp:  350 | Ep_r:  250.95\nEp:  351 | Ep_r:  625.88\nEp:  352 | Ep_r:  417.7\nEp:  353 | Ep_r:  867.81\nEp:  354 | Ep_r:  150.62\nEp:  355 | Ep_r:  230.89\nEp:  356 | Ep_r:  1017.52\nEp:  357 | Ep_r:  190.28\nEp:  358 | Ep_r:  396.91\nEp:  359 | Ep_r:  305.53\nEp:  360 | Ep_r:  131.61\nEp:  361 | Ep_r:  387.54\nEp:  362 | Ep_r:  298.82\nEp:  363 | Ep_r:  207.56\nEp:  364 | Ep_r:  248.56\nEp:  365 | Ep_r:  589.12\nEp:  366 | Ep_r:  179.52\nEp:  367 | Ep_r:  130.19\nEp:  368 | Ep_r:  1220.84\nEp:  369 | Ep_r:  126.35\nEp:  370 | Ep_r:  133.31\nEp:  371 | Ep_r:  485.81\nEp:  372 | Ep_r:  823.4\nEp:  373 | Ep_r:  253.26\nEp:  374 | Ep_r:  466.06\nEp:  375 | Ep_r:  203.27\nEp:  376 | Ep_r:  386.5\nEp:  377 | Ep_r:  491.02\nEp:  378 | Ep_r:  239.45\nEp:  379 | Ep_r:  276.93\nEp:  380 | Ep_r:  331.98\nEp:  381 | Ep_r:  764.79\nEp:  382 | Ep_r:  198.29\nEp:  383 | Ep_r:  717.18\nEp:  384 | Ep_r:  562.15\nEp:  385 | Ep_r:  29.44\nEp:  386 | Ep_r:  344.95\nEp:  387 | Ep_r:  671.87\nEp:  388 | Ep_r:  299.81\nEp:  389 | Ep_r:  899.76\nEp:  390 | Ep_r:  319.04\nEp:  391 | Ep_r:  252.11\nEp:  392 | Ep_r:  865.62\nEp:  393 | Ep_r:  255.64\nEp:  394 | Ep_r:  81.74\nEp:  395 | Ep_r:  213.13\nEp:  396 | Ep_r:  422.33\nEp:  397 | Ep_r:  167.47\nEp:  398 | Ep_r:  507.34\nEp:  399 | Ep_r:  614.0\n")),Object(E.b)("pre",null,Object(E.b)("code",Object(p.a)({parentName:"pre"},{className:"language-python"}),"")))}i.isMDXComponent=!0}}]);